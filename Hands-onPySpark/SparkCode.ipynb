{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce3768b",
   "metadata": {},
   "source": [
    "## Create Spark session and import requrired libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bda7021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: pyspark>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from delta-spark) (4.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from delta-spark) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.0.0->delta-spark) (3.23.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark>=4.0.0->delta-spark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0bebd",
   "metadata": {},
   "source": [
    "## Create Schema and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88dce2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee Schema\n",
    "emp_schema = StructType([\n",
    "    StructField(\"EmpID\", IntegerType(), True),\n",
    "    StructField(\"EmpName\", StringType(), True),\n",
    "    StructField(\"DeptID\", IntegerType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"JoiningDate\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True)\n",
    "])\n",
    "\n",
    "emp_data = [\n",
    "    (1, \"Amit\", 101, 70000, \"2020-01-10\", \"M\"),\n",
    "    (2, \"Priya\", 102, 80000, \"2019-03-15\", \"F\"),\n",
    "    (3, \"Rohit\", 101, 90000, \"2021-06-01\", \"M\"),\n",
    "    (4, \"Sneha\", 103, 60000, \"2020-11-20\", \"F\"),\n",
    "    (5, \"Ankit\", 102, 75000, \"2022-02-10\", \"M\"),\n",
    "    (6, \"Riya\", 101, 72000, \"2023-01-05\", \"F\"),\n",
    "    (7, \"Dev\", 103, 67000, \"2020-09-23\", \"M\"),\n",
    "    (8, \"Simran\", 102, 81000, \"2021-05-25\", \"F\"),\n",
    "    (9, \"Karan\", 101, 95000, \"2018-12-11\", \"M\"),\n",
    "    (10, \"Neha\", 103, 58000, \"2023-04-30\", \"F\")\n",
    "]\n",
    "\n",
    "df_emp = spark.createDataFrame(emp_data, schema=emp_schema)\n",
    "\n",
    "# Department Schema\n",
    "dept_schema = StructType([\n",
    "    StructField(\"DeptID\", IntegerType(), True),\n",
    "    StructField(\"DeptName\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True)\n",
    "])\n",
    "\n",
    "dept_data = [\n",
    "    (101, \"IT\", \"Bangalore\"),\n",
    "    (102, \"HR\", \"Hyderabad\"),\n",
    "    (103, \"Finance\", \"Pune\")\n",
    "]\n",
    "\n",
    "df_dept = spark.createDataFrame(dept_data, schema=dept_schema)\n",
    "\n",
    "# Sales Data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"SaleID\", IntegerType(), True),\n",
    "    StructField(\"EmpID\", IntegerType(), True),\n",
    "    StructField(\"SaleAmount\", IntegerType(), True),\n",
    "    StructField(\"SaleDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "sales_data = [\n",
    "    (1, 1, 1000, \"2024-01-10\"),\n",
    "    (2, 2, 2000, \"2024-01-12\"),\n",
    "    (3, 3, 1500, \"2024-02-01\"),\n",
    "    (4, 1, 2500, \"2024-02-15\"),\n",
    "    (5, 4, 3000, \"2024-03-20\"),\n",
    "    (6, 2, 1800, \"2024-03-22\"),\n",
    "    (7, 6, 2200, \"2024-04-10\"),\n",
    "    (8, 9, 4000, \"2024-04-15\")\n",
    "]\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, schema=sales_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f3b2d",
   "metadata": {},
   "source": [
    "## Beginner Level (1‚Äì15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17b34e",
   "metadata": {},
   "source": [
    "### 1Ô∏è Show first 5 records of employee dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4409bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_emp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f5818",
   "metadata": {},
   "source": [
    "### 2Ô∏è Display schema of employee DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15870818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmpID: integer (nullable = true)\n",
      " |-- EmpName: string (nullable = true)\n",
      " |-- DeptID: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- JoiningDate: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9304309",
   "metadata": {},
   "source": [
    "### 3Ô∏è Select only EmpName and Salary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744d4e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|EmpName|Salary|\n",
      "+-------+------+\n",
      "|   Amit| 70000|\n",
      "|  Priya| 80000|\n",
      "|  Rohit| 90000|\n",
      "|  Sneha| 60000|\n",
      "|  Ankit| 75000|\n",
      "|   Riya| 72000|\n",
      "|    Dev| 67000|\n",
      "| Simran| 81000|\n",
      "|  Karan| 95000|\n",
      "|   Neha| 58000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.select('EmpName','Salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3358a",
   "metadata": {},
   "source": [
    "### 4Ô∏è Filter employees with salary > 75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6883f92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.filter(df_emp.Salary >75000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e8bb7",
   "metadata": {},
   "source": [
    "### 5Ô∏è Count total number of employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28195ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c4fed",
   "metadata": {},
   "source": [
    "### 6Ô∏è Find distinct department IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d398a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|DeptID|\n",
      "+------+\n",
      "|   101|\n",
      "|   103|\n",
      "|   102|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.select(\"DeptID\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90089cf",
   "metadata": {},
   "source": [
    "### 7Ô∏è Sort employees by salary descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75cae15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.orderBy(col(\"Salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2cc93b",
   "metadata": {},
   "source": [
    "### 8Ô∏è Add a new column Bonus = 10% of Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26952881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender| Bonus|\n",
      "+-----+-------+------+------+-----------+------+------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|7000.0|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|8000.0|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|9000.0|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|6000.0|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|7500.0|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|7200.0|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|6700.0|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|8100.0|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|9500.0|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|5800.0|\n",
      "+-----+-------+------+------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.withColumn(\"Bonus\", col(\"Salary\")*0.1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9ea58",
   "metadata": {},
   "source": [
    "### 9Ô∏è Rename column ‚ÄúEmpName‚Äù to ‚ÄúEmployeeName‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288e9b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+------+-----------+------+\n",
      "|EmpID|EmployeeName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+------------+------+------+-----------+------+\n",
      "|    1|        Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    2|       Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|       Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    4|       Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    5|       Ankit|   102| 75000| 2022-02-10|     M|\n",
      "|    6|        Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    7|         Dev|   103| 67000| 2020-09-23|     M|\n",
      "|    8|      Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    9|       Karan|   101| 95000| 2018-12-11|     M|\n",
      "|   10|        Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+------------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.withColumnRenamed(\"EmpName\", \"EmployeeName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48025c",
   "metadata": {},
   "source": [
    "### Find maximum & minimum salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "352aabf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Max_Salary|\n",
      "+----------+\n",
      "|     95000|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|Min_Salary|\n",
      "+----------+\n",
      "|     58000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.agg(max(\"Salary\").alias(\"Max_Salary\")).show()\n",
    "\n",
    "df_emp.agg(min(\"Salary\").alias(\"Min_Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d177bd2",
   "metadata": {},
   "source": [
    "### 11Ô∏è Group by DeptID and find average salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b61a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|DeptID|        Avg_Salary|\n",
      "+------+------------------+\n",
      "|   101|           81750.0|\n",
      "|   103|61666.666666666664|\n",
      "|   102| 78666.66666666667|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.groupBy('DeptID').agg(avg('Salary').alias('Avg_Salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd84ba",
   "metadata": {},
   "source": [
    "### 12Ô∏è Filter female employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab58cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.filter(col('Gender') == 'F').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ad0f0",
   "metadata": {},
   "source": [
    "### 13Ô∏è Show number of employees by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1cea33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|Gender|Num_Employees|\n",
      "+------+-------------+\n",
      "|     F|            5|\n",
      "|     M|            5|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.groupBy(col('Gender')).agg(count(col(\"EmpID\")).alias(\"Num_Employees\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b29dd6",
   "metadata": {},
   "source": [
    "### 14Ô∏è Add column ‚ÄúYearsOfExperience‚Äù based on JoiningDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52d6e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+-----------------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|YearsOfExperience|\n",
      "+-----+-------+------+------+-----------+------+-----------------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|             5.91|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|             6.73|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|             4.52|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|             5.05|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|             3.83|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|             2.92|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|             5.21|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|             4.53|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|             6.99|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|              2.6|\n",
      "+-----+-------+------+------+-----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.withColumn(\"YearsOfExperience\", round(months_between(current_date(), to_date(col(\"JoiningDate\")))/12, 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb9f6a",
   "metadata": {},
   "source": [
    "### 15Ô∏è Combine department name with employee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4efd3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+------+-----------+------+--------+---------+\n",
      "|DeptID|EmpID|EmpName|Salary|JoiningDate|Gender|DeptName| Location|\n",
      "+------+-----+-------+------+-----------+------+--------+---------+\n",
      "|   101|    1|   Amit| 70000| 2020-01-10|     M|      IT|Bangalore|\n",
      "|   101|    3|  Rohit| 90000| 2021-06-01|     M|      IT|Bangalore|\n",
      "|   101|    6|   Riya| 72000| 2023-01-05|     F|      IT|Bangalore|\n",
      "|   101|    9|  Karan| 95000| 2018-12-11|     M|      IT|Bangalore|\n",
      "|   102|    2|  Priya| 80000| 2019-03-15|     F|      HR|Hyderabad|\n",
      "|   102|    5|  Ankit| 75000| 2022-02-10|     M|      HR|Hyderabad|\n",
      "|   102|    8| Simran| 81000| 2021-05-25|     F|      HR|Hyderabad|\n",
      "|   103|    4|  Sneha| 60000| 2020-11-20|     F| Finance|     Pune|\n",
      "|   103|    7|    Dev| 67000| 2020-09-23|     M| Finance|     Pune|\n",
      "|   103|   10|   Neha| 58000| 2023-04-30|     F| Finance|     Pune|\n",
      "+------+-----+-------+------+-----------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.join(df_dept, on='DeptID', how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7058b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[EmpID#0,EmpName#1,DeptID#2,Salary#3,JoiningDate#4,Gender#5]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9381a5",
   "metadata": {},
   "source": [
    "## Intermediate Level (16‚Äì35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9a5cd",
   "metadata": {},
   "source": [
    "### 16Ô∏è Get highest-paid employee in each departmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd54f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DeptID: integer (nullable = true)\n",
      " |-- DeptName: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dept.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fcda10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Dept_Id|Max_salary|\n",
      "+-------+----------+\n",
      "|    101|     95000|\n",
      "|    103|     67000|\n",
      "|    102|     81000|\n",
      "+-------+----------+\n",
      "\n",
      "+------+----------+--------+---------+\n",
      "|DeptID|Max_salary|DeptName| Location|\n",
      "+------+----------+--------+---------+\n",
      "|   101|     95000|      IT|Bangalore|\n",
      "|   103|     67000| Finance|     Pune|\n",
      "|   102|     81000|      HR|Hyderabad|\n",
      "+------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Higest_Pay_Emp = df_emp.groupBy(F.col('DeptID').alias('Dept_Id')).agg(F.max(F.col('Salary')).alias('Max_salary'))\n",
    "df_Higest_Pay_Emp.show()\n",
    "\n",
    "#### Join with department table\n",
    "df_Higest_Pay_Emp_dept = df_Higest_Pay_Emp.join(df_dept, df_Higest_Pay_Emp['Dept_Id'] == df_dept['DeptID'], 'inner')\n",
    "result = df_Higest_Pay_Emp_dept.select(F.col('DeptID'), F.col('Max_salary'), F.col('DeptName'),F.col('Location'))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7561b6",
   "metadata": {},
   "source": [
    "#### 17Ô∏è Find employees with name starting with ‚ÄúS‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72b69af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_S_emp = df_emp.filter(F.col('EmpName').startswith(\"S\"))\n",
    "\n",
    "df_S_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8533ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SaleID: integer (nullable = true)\n",
      " |-- EmpID: integer (nullable = true)\n",
      " |-- SaleAmount: integer (nullable = true)\n",
      " |-- SaleDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595690be",
   "metadata": {},
   "source": [
    "#### 18Ô∏è Find total sales amount per employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e09e7ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|EmpID|Total_Sales_Amount|\n",
      "+-----+------------------+\n",
      "|    1|              3500|\n",
      "|    3|              1500|\n",
      "|    2|              3800|\n",
      "|    6|              2200|\n",
      "|    9|              4000|\n",
      "|    4|              3000|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total_sales_per_emp = df_sales.groupBy(F.col('EmpID')).agg(F.sum(F.col('SaleAmount')).alias('Total_Sales_Amount'))\n",
    "df_total_sales_per_emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed28017",
   "metadata": {},
   "source": [
    "#### 19 Join employee and sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6836ac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+------+----------+----------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|SaleID|SaleAmount|  SaleDate|\n",
      "+-----+-------+------+------+-----------+------+------+----------+----------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|     1|      1000|2024-01-10|\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|     4|      2500|2024-02-15|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|     2|      2000|2024-01-12|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|     6|      1800|2024-03-22|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|     3|      1500|2024-02-01|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|     5|      3000|2024-03-20|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|     7|      2200|2024-04-10|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|     8|      4000|2024-04-15|\n",
      "+-----+-------+------+------+-----------+------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combine_emp_sales = df_emp.join(df_sales, df_emp['EmpID'] == df_sales['EmpID'], 'inner').drop(df_sales['EmpID'])\n",
    "df_combine_emp_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd39c68",
   "metadata": {},
   "source": [
    "#### 20Ô∏è Find employees who have not made any sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45f67269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_not_sale_emp = df_emp.join(df_sales, df_emp['EmpID'] == df_sales['EmpID'], 'left_anti')\n",
    "df_not_sale_emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301115fa",
   "metadata": {},
   "source": [
    "#### 21Ô∏è Add ‚ÄúPerformanceCategory‚Äù column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c1e11a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+---------------------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|‚ÄúPerformanceCategory‚Äù|\n",
      "+-----+-------+------+------+-----------+------+---------------------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|                  Low|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|               Medium|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|               Medium|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|                  Low|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|               Medium|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|               Medium|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|                  Low|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|               Medium|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|                 High|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|                  Low|\n",
      "+-----+-------+------+------+-----------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df_emp = df_emp.withColumn('‚ÄúPerformanceCategory‚Äù', \n",
    "                               F.when(F.col('Salary')> 90000, 'High')\\\n",
    "                               .when(F.col('Salary')>70000, 'Medium')\\\n",
    "                               .otherwise('Low')                               \n",
    "                               )\n",
    "new_df_emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae045f6",
   "metadata": {},
   "source": [
    "#### 22Ô∏è Convert all employee names to uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6ff08c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+-------------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|EmpName_upper|\n",
      "+-----+-------+------+------+-----------+------+-------------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|         AMIT|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|        PRIYA|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|        ROHIT|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|        SNEHA|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|        ANKIT|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|         RIYA|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|          DEV|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|       SIMRAN|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|        KARAN|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|         NEHA|\n",
      "+-----+-------+------+------+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_upper = df_emp.withColumn('EmpName_upper', F.upper(F.col('EmpName')))\n",
    "df_emp_upper.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af9458",
   "metadata": {},
   "source": [
    "#### 23Ô∏è Find total sales per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "208362ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+----------+\n",
      "|SaleID|EmpID|SaleAmount|  SaleDate|\n",
      "+------+-----+----------+----------+\n",
      "|     1|    1|      1000|2024-01-10|\n",
      "|     2|    2|      2000|2024-01-12|\n",
      "|     3|    3|      1500|2024-02-01|\n",
      "|     4|    1|      2500|2024-02-15|\n",
      "|     5|    4|      3000|2024-03-20|\n",
      "|     6|    2|      1800|2024-03-22|\n",
      "|     7|    6|      2200|2024-04-10|\n",
      "|     8|    9|      4000|2024-04-15|\n",
      "+------+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faa792ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|Sales_by_Month|Total_sales|\n",
      "+--------------+-----------+\n",
      "|             1|       3000|\n",
      "|             2|       4000|\n",
      "|             3|       4800|\n",
      "|             4|       6200|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_monthly_sales = df_sales.groupBy(F.month(F.col('SaleDate')).alias('Sales_by_Month')).agg(F.sum(F.col('SaleAmount')).alias('Total_sales'))\n",
    "df_monthly_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447cc88",
   "metadata": {},
   "source": [
    "#### 24Ô∏è Find top 3 highest earners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3dc2918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topEarners =  df_emp.orderBy(F.col('salary').desc()).limit(3)\n",
    "topEarners.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeed7760",
   "metadata": {},
   "source": [
    "### 25Ô∏è Create a window to rank employees by salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3dabc273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+------------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|SalaryRanked|\n",
      "+-----+-------+------+------+-----------+------+------------+\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|           1|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|           2|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|           3|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|           4|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|           5|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|           6|\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|           7|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|           8|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|           9|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|          10|\n",
      "+-----+-------+------+------+-----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = Window.orderBy(F.col('Salary').desc())\n",
    "df_emp_rnked = df_emp.withColumn('SalaryRanked', F.dense_rank().over(w))\n",
    "df_emp_rnked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a9978",
   "metadata": {},
   "source": [
    "#### 26Ô∏è Rank employees within each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b95a82e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+--------------------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|Dept_WiseSalary_Rank|\n",
      "+-----+-------+------+------+-----------+------+--------------------+\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|                   1|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|                   2|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|                   3|\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|                   4|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|                   1|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|                   2|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|                   3|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|                   1|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|                   2|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|                   3|\n",
      "+-----+-------+------+------+-----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(F.col('DeptID')).orderBy(F.desc(F.col('Salary')))\n",
    "\n",
    "df_rnk_salary_dept = df_emp.withColumn('Dept_WiseSalary_Rank', F.dense_rank().over(w)).orderBy(F.col('DeptID'))\n",
    "df_rnk_salary_dept.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052943b",
   "metadata": {},
   "source": [
    "#### 28Ô∏è Find department with highest total salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fad75ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|DeptID|Total_Amount|\n",
      "+------+------------+\n",
      "|   103|      185000|\n",
      "|   102|      236000|\n",
      "|   101|      327000|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dept_high = df_emp.groupBy(F.col('DeptID')).agg(F.sum(F.col('Salary')).alias('Total_Amount')).orderBy(F.col('Total_Amount'))\n",
    "df_dept_high.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df250fec",
   "metadata": {},
   "source": [
    "#### 29 Calculate average sales per employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96a89fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+----------+\n",
      "|SaleID|EmpID|SaleAmount|  SaleDate|\n",
      "+------+-----+----------+----------+\n",
      "|     1|    1|      1000|2024-01-10|\n",
      "|     2|    2|      2000|2024-01-12|\n",
      "|     3|    3|      1500|2024-02-01|\n",
      "|     4|    1|      2500|2024-02-15|\n",
      "|     5|    4|      3000|2024-03-20|\n",
      "|     6|    2|      1800|2024-03-22|\n",
      "|     7|    6|      2200|2024-04-10|\n",
      "|     8|    9|      4000|2024-04-15|\n",
      "+------+-----+----------+----------+\n",
      "\n",
      "+-----+----------------+\n",
      "|EmpID|AvarageSales_emp|\n",
      "+-----+----------------+\n",
      "|    1|          1750.0|\n",
      "|    3|          1500.0|\n",
      "|    2|          1900.0|\n",
      "|    6|          2200.0|\n",
      "|    9|          4000.0|\n",
      "|    4|          3000.0|\n",
      "+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show()\n",
    "avg_sales_emp = df_sales.groupBy('EmpID').agg(F.avg('SaleAmount').alias('AvarageSales_emp'))\n",
    "avg_sales_emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f312969",
   "metadata": {},
   "source": [
    "#### 30Ô∏è Find duplicate department IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51cb4aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|EmpID|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getDuplicate = df_emp.groupBy(F.col('EmpID')).count().filter('count > 1')\n",
    "getDuplicate.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71fa55f",
   "metadata": {},
   "source": [
    "### 31Ô∏è Calculate salary percentile by department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69e19524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b096cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+------------------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|       PercentRank|\n",
      "+-----+-------+------+------+-----------+------+------------------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|               0.0|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|0.3333333333333333|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|0.6666666666666666|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|               1.0|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|               0.0|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|               0.5|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|               1.0|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|               0.0|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|               0.5|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|               1.0|\n",
      "+-----+-------+------+------+-----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(F.col('DeptID')).orderBy(F.col('Salary'))\n",
    "df_percentile = df_emp.withColumn('PercentRank', F.percent_rank().over(w))\n",
    "df_percentile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f6a17",
   "metadata": {},
   "source": [
    "### 32 Add column indicating whether the employee has any sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c598cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SaleID: integer (nullable = true)\n",
      " |-- EmpID: integer (nullable = true)\n",
      " |-- SaleAmount: integer (nullable = true)\n",
      " |-- SaleDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c8584f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n",
      "+-----+-------+------+------+-----------+------+--------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|HasSales|\n",
      "+-----+-------+------+------+-----------+------+--------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|   false|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|   false|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|   false|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|   false|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|   false|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|   false|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|   false|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|   false|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|   false|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|   false|\n",
      "+-----+-------+------+------+-----------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_em_sales = df_emp.join(df_sales.select(\"EmpID\").distinct(), \"EmpId\", 'left')\n",
    "df_em_sales.show()\n",
    "\n",
    "cal_df = df_em_sales.withColumn(\"HasSales\", F.when(F.col('EmpID').isNull(), True).otherwise(False))\n",
    "cal_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688a2f0",
   "metadata": {},
   "source": [
    "### 33Ô∏è Replace null values with defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c50463dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df = df_emp.fillna({\"Salary\": 50000})\n",
    "null_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff31e1",
   "metadata": {},
   "source": [
    "### 34Ô∏è Calculate cumulative sales per employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c757d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SaleID: integer (nullable = true)\n",
      " |-- EmpID: integer (nullable = true)\n",
      " |-- SaleAmount: integer (nullable = true)\n",
      " |-- SaleDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44dd17f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+----------+---------------+\n",
      "|SaleID|EmpID|SaleAmount|  SaleDate|cumulativeSales|\n",
      "+------+-----+----------+----------+---------------+\n",
      "|     1|    1|      1000|2024-01-10|           1000|\n",
      "|     4|    1|      2500|2024-02-15|           3500|\n",
      "|     2|    2|      2000|2024-01-12|           2000|\n",
      "|     6|    2|      1800|2024-03-22|           3800|\n",
      "|     3|    3|      1500|2024-02-01|           1500|\n",
      "|     5|    4|      3000|2024-03-20|           3000|\n",
      "|     7|    6|      2200|2024-04-10|           2200|\n",
      "|     8|    9|      4000|2024-04-15|           4000|\n",
      "+------+-----+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win = Window.partitionBy('EmpID').orderBy('SaleDate')\n",
    "\n",
    "cum_df = df_sales.withColumn('cumulativeSales', F.sum('SaleAmount').over(win))\n",
    "\n",
    "cum_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9be813",
   "metadata": {},
   "source": [
    "### 35Ô∏è Pivot sales data by month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e16fce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+----+----+\n",
      "|EmpID|   1|   2|   3|   4|\n",
      "+-----+----+----+----+----+\n",
      "|    1|1000|2500|NULL|NULL|\n",
      "|    6|NULL|NULL|NULL|2200|\n",
      "|    3|NULL|1500|NULL|NULL|\n",
      "|    9|NULL|NULL|NULL|4000|\n",
      "|    4|NULL|NULL|3000|NULL|\n",
      "|    2|2000|NULL|1800|NULL|\n",
      "+-----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_pi = df_sales.withColumn(\"Month\", F.month('SaleDate')).groupBy('EmpID').pivot('Month').agg(F.sum('SaleAmount'))\n",
    "df_sales_pi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b0ce2",
   "metadata": {},
   "source": [
    "# üî¥ Advanced Level (36‚Äì50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ddb05",
   "metadata": {},
   "source": [
    "#### 36Ô∏è Optimize joins using broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3077ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+------+-----------+------+--------+---------+\n",
      "|DeptID|EmpID|EmpName|Salary|JoiningDate|Gender|DeptName| Location|\n",
      "+------+-----+-------+------+-----------+------+--------+---------+\n",
      "|   101|    1|   Amit| 70000| 2020-01-10|     M|      IT|Bangalore|\n",
      "|   102|    2|  Priya| 80000| 2019-03-15|     F|      HR|Hyderabad|\n",
      "|   101|    3|  Rohit| 90000| 2021-06-01|     M|      IT|Bangalore|\n",
      "|   103|    4|  Sneha| 60000| 2020-11-20|     F| Finance|     Pune|\n",
      "|   102|    5|  Ankit| 75000| 2022-02-10|     M|      HR|Hyderabad|\n",
      "|   101|    6|   Riya| 72000| 2023-01-05|     F|      IT|Bangalore|\n",
      "|   103|    7|    Dev| 67000| 2020-09-23|     M| Finance|     Pune|\n",
      "|   102|    8| Simran| 81000| 2021-05-25|     F|      HR|Hyderabad|\n",
      "|   101|    9|  Karan| 95000| 2018-12-11|     M|      IT|Bangalore|\n",
      "|   103|   10|   Neha| 58000| 2023-04-30|     F| Finance|     Pune|\n",
      "+------+-----+-------+------+-----------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df_emp.join(F.broadcast(df_dept), 'DeptID')\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88528ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [DeptID#2, EmpID#0, EmpName#1, Salary#3, JoiningDate#4, Gender#5, DeptName#7, Location#8]\n",
      "   +- BroadcastHashJoin [DeptID#2], [DeptID#6], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(DeptID#2)\n",
      "      :  +- Scan ExistingRDD[EmpID#0,EmpName#1,DeptID#2,Salary#3,JoiningDate#4,Gender#5]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1748]\n",
      "         +- Filter isnotnull(DeptID#6)\n",
      "            +- Scan ExistingRDD[DeptID#6,DeptName#7,Location#8]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d5c69",
   "metadata": {},
   "source": [
    "### 37Ô∏è Repartition data by DeptID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f47e04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange hashpartitioning(DeptID#2, 200), REPARTITION_BY_COL, [plan_id=1757]\n",
      "   +- Scan ExistingRDD[EmpID#0,EmpName#1,DeptID#2,Salary#3,JoiningDate#4,Gender#5]\n",
      "\n",
      "\n",
      "+------+-----+\n",
      "|DeptID|count|\n",
      "+------+-----+\n",
      "|   101|    4|\n",
      "|   103|    3|\n",
      "|   102|    3|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repart = df_emp.repartition(\"DeptID\")\n",
    "repart.explain()\n",
    "\n",
    "repart.groupBy('DeptID').count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4679b",
   "metadata": {},
   "source": [
    "### 38Ô∏è Cache DataFrame for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e615312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.cache()\n",
    "\n",
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abebf5",
   "metadata": {},
   "source": [
    "### 39Ô∏è Write employee data as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b4902d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.write.format('parquet').mode('overwrite').save('./temp/employee_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf207f",
   "metadata": {},
   "source": [
    "### 40Ô∏è Read data from Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d52ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|\n",
      "+-----+-------+------+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet('./temp/employee_parquet')\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd93a59",
   "metadata": {},
   "source": [
    "### 41Ô∏è Handle schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22652966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_emp.withColumn('Country', F.lit('India'))\n",
    "df_new.write.format('parquet').mode('overwrite').option('mergeSchema', True).save('./temp/employee_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c5513a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+------+-----------+------+-------+\n",
      "|EmpID|EmpName|DeptID|Salary|JoiningDate|Gender|Country|\n",
      "+-----+-------+------+------+-----------+------+-------+\n",
      "|    1|   Amit|   101| 70000| 2020-01-10|     M|  India|\n",
      "|    2|  Priya|   102| 80000| 2019-03-15|     F|  India|\n",
      "|    3|  Rohit|   101| 90000| 2021-06-01|     M|  India|\n",
      "|    4|  Sneha|   103| 60000| 2020-11-20|     F|  India|\n",
      "|    5|  Ankit|   102| 75000| 2022-02-10|     M|  India|\n",
      "|    6|   Riya|   101| 72000| 2023-01-05|     F|  India|\n",
      "|    7|    Dev|   103| 67000| 2020-09-23|     M|  India|\n",
      "|    8| Simran|   102| 81000| 2021-05-25|     F|  India|\n",
      "|    9|  Karan|   101| 95000| 2018-12-11|     M|  India|\n",
      "|   10|   Neha|   103| 58000| 2023-04-30|     F|  India|\n",
      "+-----+-------+------+------+-----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b22079",
   "metadata": {},
   "source": [
    "### 42Ô∏è Create Delta table from employee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "240e0af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o475.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:681)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.classic.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:626)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:135)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:665)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:665)\n\tat scala.util.Failure.orElse(Try.scala:230)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:665)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2098962925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_emp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./output/delta/employee_delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1743\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o475.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:681)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.classic.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:626)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:135)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:665)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:665)\n\tat scala.util.Failure.orElse(Try.scala:230)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:665)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "df_emp.write.format(\"delta\").mode(\"overwrite\").save(\"./output/delta/employee_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c183ca",
   "metadata": {},
   "source": [
    "### 43Ô∏è Update Delta table (SCD Type 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98b0e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
