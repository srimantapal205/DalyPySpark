{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d70c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *  # Import the function\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql.functions import col, explode, array_repeat\n",
    "import json\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491bced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------+--------+\n",
      "|customers            |items                       |order_id|\n",
      "+---------------------+----------------------------+--------+\n",
      "|{50, Bangalore, Riya}|[{100, 2, A1}, {200, 1, B1}]|1001    |\n",
      "+---------------------+----------------------------+--------+\n",
      "\n",
      "root\n",
      " |-- customers: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- price: long (nullable = true)\n",
      " |    |    |-- qty: long (nullable = true)\n",
      " |    |    |-- sku: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[customers#10,items#11,order_id#12L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"order_id\": 1001,\n",
    "    \"customers\": {\"id\": 50, \"name\": \"Riya\", \"location\": \"Bangalore\"},\n",
    "    \"items\": [\n",
    "        {\"sku\": \"A1\", \"qty\": 2, \"price\": 100},\n",
    "        {\"sku\": \"B1\", \"qty\": 1, \"price\": 200}\n",
    "    ]\n",
    "}\n",
    "\n",
    "json_rdd = spark.sparkContext.parallelize([json.dumps(data)])\n",
    "#df = spark.createDataFrame([data])\n",
    "df = spark.read.json(json_rdd)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()\n",
    "df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7750c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+------------+\n",
      "|order_id|customer_id|customer_name|item        |\n",
      "+--------+-----------+-------------+------------+\n",
      "|1001    |50         |Riya         |{100, 2, A1}|\n",
      "|1001    |50         |Riya         |{200, 1, B1}|\n",
      "+--------+-----------+-------------+------------+\n",
      "\n",
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- item: struct (nullable = true)\n",
      " |    |-- price: long (nullable = true)\n",
      " |    |-- qty: long (nullable = true)\n",
      " |    |-- sku: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\n",
    "    col('order_id'),\n",
    "    col('customers.id').alias('customer_id'),\n",
    "    col('customers.name').alias('customer_name'),\n",
    "    explode(col('items')).alias('item')\n",
    ")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26519dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+--------+--------+----------+\n",
      "|order_id|customer_id|customer_name|item_sku|item_qty|item_price|\n",
      "+--------+-----------+-------------+--------+--------+----------+\n",
      "|1001    |50         |Riya         |A1      |2       |100       |\n",
      "|1001    |50         |Riya         |B1      |1       |200       |\n",
      "+--------+-----------+-------------+--------+--------+----------+\n",
      "\n",
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- item_sku: string (nullable = true)\n",
      " |-- item_qty: long (nullable = true)\n",
      " |-- item_price: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= df.select(\n",
    "    col('order_id'),\n",
    "    col('customer_id'),\n",
    "    col('customer_name'),\n",
    "    col('item.sku').alias('item_sku'),\n",
    "    col('item.qty').alias('item_qty'),\n",
    "    col('item.price').alias('item_price')\n",
    ")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f73fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"Orders\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a9a17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1781d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+\n",
      "|ItemName|Quantity|TotalPrice|\n",
      "+--------+--------+----------+\n",
      "| Monitor|       2|     20000|\n",
      "|     CPU|       3|     90000|\n",
      "|Earphone|       4|      6000|\n",
      "+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Monitor\", 2, 20000),\n",
    "    (\"CPU\", 3, 90000),\n",
    "    (\"Earphone\", 4, 6000)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"ItemName\", \"Quantity\", \"TotalPrice\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ca6ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|ItemName|Quantity|UnitPrice|\n",
      "+--------+--------+---------+\n",
      "| Monitor|       1|  10000.0|\n",
      "| Monitor|       1|  10000.0|\n",
      "|     CPU|       1|  30000.0|\n",
      "|     CPU|       1|  30000.0|\n",
      "|     CPU|       1|  30000.0|\n",
      "|Earphone|       1|   1500.0|\n",
      "|Earphone|       1|   1500.0|\n",
      "|Earphone|       1|   1500.0|\n",
      "|Earphone|       1|   1500.0|\n",
      "+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.functions import col, explode, array_repeat\n",
    "df_result = (df\n",
    "             .withColumn(\"UnitPrice\", col('TotalPrice')/col('Quantity'))\n",
    "             .withColumn(\"dummy_column\", \n",
    "                         explode(\n",
    "                             array_repeat(\n",
    "                                 col(\"UnitPrice\"), \n",
    "                                 col(\"Quantity\").cast(\"int\")\n",
    "                                )\n",
    "                            )\n",
    "                         )\n",
    "             .select(\"ItemName\", col('dummy_column').alias(\"UnitPrice\"))\n",
    "             .withColumn('Quantity', lit(1))\n",
    "             .select(\"ItemName\", \"Quantity\", \"UnitPrice\")            \n",
    ")\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a6bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n",
      "|ItemName|Quantity|UnitPrice|\n",
      "+--------+--------+---------+\n",
      "| Monitor|       2|  20000.0|\n",
      "|     CPU|       3|  90000.0|\n",
      "|Earphone|       4|   6000.0|\n",
      "+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df = df_result.groupBy('ItemName').agg(\n",
    "\tsum(col('Quantity')).alias('Quantity'),\n",
    "\tsum(col('UnitPrice')).alias('UnitPrice')\n",
    ")\n",
    "\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d432b8d3",
   "metadata": {},
   "source": [
    "### 1) Extract the date from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6296fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            filename|\n",
      "+--------------------+\n",
      "|passenger_2025112...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+--------+\n",
      "|            filename|    date|\n",
      "+--------------------+--------+\n",
      "|passenger_2025112...|20251125|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "data = [(\"passenger_20251125.csv\",)] \n",
    "df = spark.createDataFrame(data, [\"filename\"])\n",
    "df.show()\n",
    "\n",
    "df_getFilename = df.select(\n",
    "    col('filename'),\n",
    "    regexp_extract(col('filename'), r'_(\\d{8})\\.csv$', 1).alias(\"date\")    \n",
    ")\n",
    "df_getFilename.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16581f",
   "metadata": {},
   "source": [
    "### 2) Pivot sales to get Product columns per Region\n",
    "\n",
    "\n",
    "|Region |Product Revenue\n",
    "|East   |A       100\n",
    "|East   |B       200\n",
    "|West   |A       150\n",
    "|West   |B       250\n",
    "|to \n",
    "|Region |  A     B\n",
    "|East   | 100   200   \n",
    "|West   | 150   250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "404359c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|Region|Category|Sales|\n",
      "+------+--------+-----+\n",
      "|  East|       A|  100|\n",
      "|  East|       B|  150|\n",
      "|  West|       A|  200|\n",
      "|  West|       B|  250|\n",
      "| North|       A|  300|\n",
      "| North|       B|  350|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('East','A',100),\n",
    "        ('East','B',150),\n",
    "        ('West','A',200),\n",
    "        ('West','B',250),\n",
    "        ('North','A',300),\n",
    "        ('North','B',350)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Region\", \"Category\", \"Sales\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "959951f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|Region|  A|  B|\n",
      "+------+---+---+\n",
      "|  East|100|150|\n",
      "|  West|200|250|\n",
      "| North|300|350|\n",
      "+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.groupBy('Region').pivot('Category').agg(sum('sales'))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f4887",
   "metadata": {},
   "source": [
    "### 3) Repeat each id N times (id repeated id times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "984d34a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|id_repeated|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          2|\n",
      "|          2|\n",
      "|          3|\n",
      "|          3|\n",
      "|          3|\n",
      "|          4|\n",
      "|          4|\n",
      "|          4|\n",
      "|          4|\n",
      "|          5|\n",
      "|          5|\n",
      "|          5|\n",
      "|          5|\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, array_repeat, col\n",
    "data = [(1,), (2,), (3,), (4,), (5,)]\n",
    "# output: 1,2,2,3,3,3,4,4,4,4,5,5,5,5,5\n",
    "df = spark.createDataFrame(data, ['id'])\n",
    "df_repeated = df.select(\n",
    "    explode(\n",
    "        array_repeat(\n",
    "            col('id'), \n",
    "            col('id').cast('int')\n",
    "        )\n",
    "    ).alias(\"id_repeated\")\n",
    "    )\n",
    "df_repeated.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412283c8",
   "metadata": {},
   "source": [
    "### 4) Generate unique team pair combinations (no duplicates, no self-pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e1809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|team_a|\n",
      "+------+\n",
      "|   RCB|\n",
      "|   SRH|\n",
      "|   CSK|\n",
      "|   KKR|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|team_b|\n",
      "+------+\n",
      "|   RCB|\n",
      "|   SRH|\n",
      "|   CSK|\n",
      "|   KKR|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "team_id,  Name\n",
    "('RCB', 'Royal challengers banglore'),\n",
    "('SRH', 'Sunrisers hydrabad'),\n",
    "('CSK', 'Chennai Super Kings'),\n",
    "('KKR', 'Kolkata Knight Riders');\n",
    "'''\n",
    "\n",
    "data = [('RCB', 'Royal challengers banglore'),\n",
    "        ('SRH', 'Sunrisers hydrabad'),      \n",
    "        ('CSK', 'Chennai Super Kings'),\n",
    "        ('KKR', 'Kolkata Knight Riders')] \n",
    "  \n",
    "df = spark.createDataFrame(data, ['team_id', 'Name'])\n",
    "df_alias_1 = df.alias('df_1').select(col('team_id').alias('team_a'))\n",
    "df_alias_2 = df.alias('df_2').select(col('team_id').alias('team_b'))\n",
    "df_alias_1.show()\n",
    "df_alias_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b564a36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|team_a|team_b|\n",
      "+------+------+\n",
      "|   RCB|   SRH|\n",
      "|   SRH|   RCB|\n",
      "|   RCB|   CSK|\n",
      "|   RCB|   KKR|\n",
      "|   SRH|   CSK|\n",
      "|   SRH|   KKR|\n",
      "|   CSK|   RCB|\n",
      "|   CSK|   SRH|\n",
      "|   KKR|   RCB|\n",
      "|   KKR|   SRH|\n",
      "|   CSK|   KKR|\n",
      "|   KKR|   CSK|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cros_join_df = df_alias_1.join(\n",
    "    df_alias_2,\n",
    "    df_alias_1['team_a'] != df_alias_2['team_b'],\n",
    "    'inner'\n",
    ")\n",
    "cros_join_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
