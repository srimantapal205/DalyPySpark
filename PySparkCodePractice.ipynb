{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark COde Practice"
      ],
      "metadata": {
        "id": "c2L-0y-LDFIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *  # Import the function\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "dVz3VjwEDKo_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial Saprk Session\n",
        "spark = SparkSession.builder.appName(\"PySparkPractice\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "_tiWFtKPDTmN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"1/1/2023\", \"C1\", 20),\n",
        "    (\"1/1/2023\", \"C2\", 20),\n",
        "    (\"1/2/2023\", \"C2\", 50),\n",
        "    (\"1/2/2023\", \"C3\", 12),\n",
        "    (\"1/3/2023\", \"C4\", 20),\n",
        "    (\"1/3/2023\", \"C5\", 100),\n",
        "    (\"1/3/2023\", \"C1\", 123),\n",
        "]\n",
        "column = ['Date_fld', 'Custome_name', 'Amount']\n",
        "\n",
        "df = spark.createDataFrame(data, column)\n",
        "df.show()\n",
        "\n",
        "# Convert date in proper format\n",
        "df = df.withColumn(\"Date_fld\", to_date(col(\"Date_fld\"), \"M/d/yyyy\"))\n",
        "df.show()\n",
        "\n",
        "# Define window partition by customer orderd by date\n",
        "window_space = Window.partitionBy('Custome_name').orderBy('Date_fld')\n",
        "\n",
        "#Assign row number within each customer number\n",
        "df = df.withColumn('Row_Number', row_number().over(window_space))\n",
        "df.show()\n",
        "\n",
        "#filter only new customer\n",
        "df_new_custome = df.filter(col('Row_Number') == 1)\n",
        "df_new_custome.show()\n",
        "\n",
        "# Count distinct new customers per date\n",
        "result_df = df_new_custome.groupBy('Date_fld').agg(countDistinct('Custome_name').alias('Price_Count'))\n",
        "result_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQ59NvmKDZ3i",
        "outputId": "4f6645f9-f5c2-4e1b-94e5-37f19eaf50fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------+------+\n",
            "|Date_fld|Custome_name|Amount|\n",
            "+--------+------------+------+\n",
            "|1/1/2023|          C1|    20|\n",
            "|1/1/2023|          C2|    20|\n",
            "|1/2/2023|          C2|    50|\n",
            "|1/2/2023|          C3|    12|\n",
            "|1/3/2023|          C4|    20|\n",
            "|1/3/2023|          C5|   100|\n",
            "|1/3/2023|          C1|   123|\n",
            "+--------+------------+------+\n",
            "\n",
            "+----------+------------+------+\n",
            "|  Date_fld|Custome_name|Amount|\n",
            "+----------+------------+------+\n",
            "|2023-01-01|          C1|    20|\n",
            "|2023-01-01|          C2|    20|\n",
            "|2023-01-02|          C2|    50|\n",
            "|2023-01-02|          C3|    12|\n",
            "|2023-01-03|          C4|    20|\n",
            "|2023-01-03|          C5|   100|\n",
            "|2023-01-03|          C1|   123|\n",
            "+----------+------------+------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-03|          C1|   123|         2|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C2|    50|         2|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+------------+------+----------+\n",
            "|  Date_fld|Custome_name|Amount|Row_Number|\n",
            "+----------+------------+------+----------+\n",
            "|2023-01-01|          C1|    20|         1|\n",
            "|2023-01-01|          C2|    20|         1|\n",
            "|2023-01-02|          C3|    12|         1|\n",
            "|2023-01-03|          C4|    20|         1|\n",
            "|2023-01-03|          C5|   100|         1|\n",
            "+----------+------------+------+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|  Date_fld|Price_Count|\n",
            "+----------+-----------+\n",
            "|2023-01-01|          2|\n",
            "|2023-01-02|          1|\n",
            "|2023-01-03|          2|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark code to solve"
      ],
      "metadata": {
        "id": "xFATEYxtcqFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Craete Spark Session\n",
        "spark2 = SparkSession\\\n",
        ".builder\\\n",
        ".config('spark.shuffle.useOldFetchers', 'true')\\\n",
        ".config('spark.ui.port','0')\\\n",
        ".config('spark.sql.warehouse.dir', '/user/itv008042/warehouse')\\\n",
        ".enableHiveSupport()\\\n",
        ".master('yarn')\\\n",
        ".appName('PySparkPractice')\\\n",
        ".getOrCreate()\n",
        "\n",
        "#Create Schema\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"ActorId\", IntegerType(), True),\n",
        "    StructField(\"DirectorId\", IntegerType(), True),\n",
        "    StructField(\"TimeStamp\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (1,1,0),\n",
        "    (1,1,1),\n",
        "    (1,1,2),\n",
        "    (1,2,3),\n",
        "    (1,2,4),\n",
        "    (1,1,5),\n",
        "    (1,1,6)\n",
        "]\n",
        "\n",
        "# Create Data frame\n",
        "df = spark2.createDataFrame(data, schema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "TfJPZ3n2ECGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a181a885-921c-4e5c-8095-67bd8decec2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+\n",
            "|ActorId|DirectorId|TimeStamp|\n",
            "+-------+----------+---------+\n",
            "|      1|         1|        0|\n",
            "|      1|         1|        1|\n",
            "|      1|         1|        2|\n",
            "|      1|         2|        3|\n",
            "|      1|         2|        4|\n",
            "|      1|         1|        5|\n",
            "|      1|         1|        6|\n",
            "+-------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Group By ActorId','DirectorId"
      ],
      "metadata": {
        "id": "YiiczhnSfO8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_group = df.groupBy('ActorId', 'DirectorId').count()\n",
        "df_group.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIjzrc2scw4h",
        "outputId": "83c57660-91b4-43d3-f02e-1045395ef650"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "|      1|         2|    2|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_group.filter(df_group['count']> 3).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgvPYNvKfnvN",
        "outputId": "5817095a-0f04-41c0-9226-807416cac9f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-----+\n",
            "|ActorId|DirectorId|count|\n",
            "+-------+----------+-----+\n",
            "|      1|         1|    5|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "02: Write an pyspark code to find the ctr of each Ad.Round ctr to 2\n",
        "decimal points. Order the result table by ctr in descending order\n",
        "and by ad_id in ascending order in case of a tie."
      ],
      "metadata": {
        "id": "JgBHsJkFicj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"ad_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"action\", StringType(), True)\n",
        "])\n",
        "data = [\n",
        "   (1, 1, 'Clicked'),\n",
        "  (2, 2, 'Clicked'),\n",
        "  (3, 3, 'Viewed'),\n",
        "  (5, 5, 'Ignored'),\n",
        "  (1, 7, 'Ignored'),\n",
        "  (2, 7, 'Viewed'),\n",
        "  (3, 5, 'Clicked'),\n",
        "  (1, 4, 'Viewed'),\n",
        "  (2, 11, 'Viewed'),\n",
        "  (1, 2, 'Clicked')\n",
        "]\n",
        "\n",
        "#Create Datframe\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFAj-6DliTqs",
        "outputId": "45aff848-2bae-4f17-bdc0-ab4e5a5e589f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+\n",
            "|ad_id|user_id| action|\n",
            "+-----+-------+-------+\n",
            "|    1|      1|Clicked|\n",
            "|    2|      2|Clicked|\n",
            "|    3|      3| Viewed|\n",
            "|    5|      5|Ignored|\n",
            "|    1|      7|Ignored|\n",
            "|    2|      7| Viewed|\n",
            "|    3|      5|Clicked|\n",
            "|    1|      4| Viewed|\n",
            "|    2|     11| Viewed|\n",
            "|    1|      2|Clicked|\n",
            "+-----+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "03: Write a Pyspark program to report the first name, last name, city, and state of each person in the\n",
        "Person dataframe. If the address of a personId is not present in the Address dataframe,\n",
        "report null instead."
      ],
      "metadata": {
        "id": "DmaD35cTq3NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define schema for the 'persons' table\n",
        "persons_schema = StructType([\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"lastName\", StringType(), True),\n",
        "  StructField(\"firstName\", StringType(), True)\n",
        "])\n",
        "# Define schema for the 'addresses' table\n",
        "addresses_schema = StructType([\n",
        "  StructField(\"addressId\", IntegerType(), True),\n",
        "  StructField(\"personId\", IntegerType(), True),\n",
        "  StructField(\"city\", StringType(), True),\n",
        "  StructField(\"state\", StringType(), True)\n",
        "])\n",
        "# Define data for the 'persons' table\n",
        "persons_data = [\n",
        "  (1, 'Wang', 'Allen'),\n",
        "  (2, 'Alice', 'Bob')\n",
        "]\n",
        "# Define data for the 'addresses' table\n",
        "addresses_data = [\n",
        "  (1, 2, 'New York City', 'New York'),\n",
        "  (2, 3, 'Leetcode', 'California')\n",
        "]\n",
        "\n",
        "#Create Data Frame\n",
        "person_df = spark.createDataFrame(persons_data, persons_schema)\n",
        "address_df = spark.createDataFrame(addresses_data, addresses_schema)\n",
        "\n",
        "person_df.show()\n",
        "address_df.show()\n"
      ],
      "metadata": {
        "id": "pYsdT38Vqt2o",
        "outputId": "fac06cb2-5a1e-4f15-94ef-5f6011dd99f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+---------+\n",
            "|personId|lastName|firstName|\n",
            "+--------+--------+---------+\n",
            "|       1|    Wang|    Allen|\n",
            "|       2|   Alice|      Bob|\n",
            "+--------+--------+---------+\n",
            "\n",
            "+---------+--------+-------------+----------+\n",
            "|addressId|personId|         city|     state|\n",
            "+---------+--------+-------------+----------+\n",
            "|        1|       2|New York City|  New York|\n",
            "|        2|       3|     Leetcode|California|\n",
            "+---------+--------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_join = person_df.join(address_df, person_df[\"personId\"] == address_df[\"personId\"], \"left\")\n",
        "df_join.select('firstName', 'lastName','city','state').show()\n",
        "\n",
        "##first name, last name, city, and state"
      ],
      "metadata": {
        "id": "vutIvI1lrWwT",
        "outputId": "4ce6e371-7d78-466f-e723-74e69efdde0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------------+--------+\n",
            "|firstName|lastName|         city|   state|\n",
            "+---------+--------+-------------+--------+\n",
            "|    Allen|    Wang|         NULL|    NULL|\n",
            "|      Bob|   Alice|New York City|New York|\n",
            "+---------+--------+-------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "04: Employees Earning More Than Their Managers\n",
        "Write a Pyspark program to find Employees Earning More Than Their\n",
        "Managers"
      ],
      "metadata": {
        "id": "DtZVuMHessY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"employees\"\n",
        "employees_schema = StructType([\n",
        "  StructField(\"id\", IntegerType(), True),\n",
        "  StructField(\"name\", StringType(), True),\n",
        "  StructField(\"salary\", IntegerType(), True),\n",
        "  StructField(\"managerId\", IntegerType(), True)\n",
        "])\n",
        "# Define data for the \"employees\"\n",
        "employees_data = [\n",
        "  (1, 'Joe', 70000, 3),\n",
        "  (2, 'Henry', 80000, 4),\n",
        "  (3, 'Sam', 60000, None),\n",
        "  (4, 'Max', 90000, None)\n",
        "]\n",
        "\n",
        "emp_df = spark.createDataFrame(employees_data, employees_schema)\n",
        "emp_df.show()\n"
      ],
      "metadata": {
        "id": "SSTRUyDCr4KP",
        "outputId": "d519777a-8699-48f8-d056-7e5fe04ceac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+---------+\n",
            "| id| name|salary|managerId|\n",
            "+---+-----+------+---------+\n",
            "|  1|  Joe| 70000|        3|\n",
            "|  2|Henry| 80000|        4|\n",
            "|  3|  Sam| 60000|     NULL|\n",
            "|  4|  Max| 90000|     NULL|\n",
            "+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_df_1 = emp_df.alias('e1')\n",
        "emp_df_2 = emp_df.alias('e2')\n",
        "\n",
        "self_joined_df = emp_df_1.join(emp_df_2, col(\"e1.id\") == col(\"e2.managerId\"))\n",
        "self_joined_df.show()\n"
      ],
      "metadata": {
        "id": "ZUS5MtKctLWM",
        "outputId": "df57aaba-5433-4181-da01-66ab2c2838f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+---------+---+-----+------+---------+\n",
            "| id|name|salary|managerId| id| name|salary|managerId|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "|  3| Sam| 60000|     NULL|  1|  Joe| 70000|        3|\n",
            "|  4| Max| 90000|     NULL|  2|Henry| 80000|        4|\n",
            "+---+----+------+---------+---+-----+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df = self_joined_df.select(col(\"e2.name\"), col('e2.salary'), col(\"e1.salary\").alias(\"manager_salary\"))\n",
        "self_joined_after_df.show()"
      ],
      "metadata": {
        "id": "pAwgyERSuYrh",
        "outputId": "623cb401-39f5-4b81-d658-bdc73174200f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------------+\n",
            "| name|salary|manager_salary|\n",
            "+-----+------+--------------+\n",
            "|  Joe| 70000|         60000|\n",
            "|Henry| 80000|         90000|\n",
            "+-----+------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self_joined_after_df.filter(self_joined_after_df.salary > self_joined_after_df.manager_salary).select('name').show()"
      ],
      "metadata": {
        "id": "ciBHiMBRvAB4",
        "outputId": "57f02df2-06ec-472f-a7fa-f85b473ee564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "| Joe|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Pyspark program to report all the duplicate emails.\n",
        "Note that it's guaranteed that the email field is not NULL."
      ],
      "metadata": {
        "id": "lxncLIOe0nlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"emails\" table\n",
        "emails_schema = StructType([\n",
        "StructField(\"id\", IntegerType(), True),\n",
        "StructField(\"email\", StringType(), True)\n",
        "])\n",
        "# Define data for the \"emails\" table\n",
        "emails_data = [\n",
        "(1, 'a@b.com'),\n",
        "(2, 'c@d.com'),\n",
        "(3, 'a@b.com')\n",
        "]\n",
        "\n",
        "# Create a PySpark DataFrame\n",
        "email_df = spark.createDataFrame(emails_data, emails_schema)\n",
        "email_df.show()\n"
      ],
      "metadata": {
        "id": "WupwVbyFvTA2",
        "outputId": "92b7a82b-81e2-409b-e467-672d1db51949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|  email|\n",
            "+---+-------+\n",
            "|  1|a@b.com|\n",
            "|  2|c@d.com|\n",
            "|  3|a@b.com|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_email_group = email_df.groupBy('email').count()\n",
        "df_email_group.show()\n",
        "\n",
        "df_email_group.filter(df_email_group['count']>1).show()"
      ],
      "metadata": {
        "id": "yhSQ-MF-1On_",
        "outputId": "69c86465-17c1-4e2a-b96b-9dafc1cc53b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "|c@d.com|    1|\n",
            "+-------+-----+\n",
            "\n",
            "+-------+-----+\n",
            "|  email|count|\n",
            "+-------+-----+\n",
            "|a@b.com|    2|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to start page 21"
      ],
      "metadata": {
        "id": "WCakKP5s2AOy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JtWqBxS1Yp-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write a program in pyspark last 3 days product sell rolling avarage price?"
      ],
      "metadata": {
        "id": "ga_2A6oe9hbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: product_id, date, price\n",
        "data = [\n",
        "    (\"p1\", \"2025-04-10\", 100),\n",
        "    (\"p1\", \"2025-04-11\", 110),\n",
        "    (\"p1\", \"2025-04-12\", 105),\n",
        "    (\"p1\", \"2025-04-13\", 120),\n",
        "    (\"p1\", \"2025-04-14\", 130),\n",
        "    (\"p2\", \"2025-04-10\", 200),\n",
        "    (\"p2\", \"2025-04-11\", 220),\n",
        "    (\"p2\", \"2025-04-12\", 210),\n",
        "    (\"p2\", \"2025-04-14\", 230),\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"date\", \"price\"])\n",
        "# Convert date to DateType\n",
        "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Define window: partition by product_id, ordered by date, looking back 2 rows (3-day window)\n",
        "windowSpace = Window.partitionBy(\"product_id\").orderBy(\"date\").rowsBetween(-2, 0)\n",
        "\n",
        "# Add rolling average column\n",
        "df_with_rolling_avg_column = df.withColumn(\"rolling_avg_price\", avg(\"price\").over(windowSpace))\n",
        "\n",
        "df_with_rolling_avg_column.orderBy(\"product_id\", \"date\").show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUBpXEBm93vg",
        "outputId": "3b69fefc-a9cd-4762-8cda-1dabc93be2c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----+\n",
            "|product_id|      date|price|\n",
            "+----------+----------+-----+\n",
            "|        p1|2025-04-10|  100|\n",
            "|        p1|2025-04-11|  110|\n",
            "|        p1|2025-04-12|  105|\n",
            "|        p1|2025-04-13|  120|\n",
            "|        p1|2025-04-14|  130|\n",
            "|        p2|2025-04-10|  200|\n",
            "|        p2|2025-04-11|  220|\n",
            "|        p2|2025-04-12|  210|\n",
            "|        p2|2025-04-14|  230|\n",
            "+----------+----------+-----+\n",
            "\n",
            "+----------+----------+-----+------------------+\n",
            "|product_id|      date|price| rolling_avg_price|\n",
            "+----------+----------+-----+------------------+\n",
            "|        p1|2025-04-10|  100|             100.0|\n",
            "|        p1|2025-04-11|  110|             105.0|\n",
            "|        p1|2025-04-12|  105|             105.0|\n",
            "|        p1|2025-04-13|  120|111.66666666666667|\n",
            "|        p1|2025-04-14|  130|118.33333333333333|\n",
            "|        p2|2025-04-10|  200|             200.0|\n",
            "|        p2|2025-04-11|  220|             210.0|\n",
            "|        p2|2025-04-12|  210|             210.0|\n",
            "|        p2|2025-04-14|  230|             220.0|\n",
            "+----------+----------+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Youâ€™re given a dataset containing employee names, departments, and their salaries.\n",
        "\n",
        " Your task is to rank the employees by salary within each department and classify them as:\n",
        "\n",
        "\"High Growth\" if the salary is higher than the previous person in that department\n",
        "\n",
        "\"Low Growth\" if the salary is lower\n",
        "\n",
        "\"No Growth\" if the salary is the same or it's the first employee in the department"
      ],
      "metadata": {
        "id": "EgPFWB8HyfBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Alice\", \"HR\", 50000),\n",
        "    (\"Bob\", \"IT\", 60000),\n",
        "    (\"Charlie\", \"HR\", 55000),\n",
        "    (\"David\", \"IT\", 70000),\n",
        "    (\"Eve\", \"HR\", 52000),\n",
        "    (\"Frank\", \"IT\", 65000),\n",
        "    (\"Grace\", \"HR\", 51000),\n",
        "    (\"Hank\", \"IT\", 68000),\n",
        "    (\"Ivy\", \"HR\", 53000),\n",
        "    (\"Jack\", \"IT\", 72000),\n",
        "    (\"Kelly\", \"HR\", 54000),\n",
        "    (\"Luke\", \"IT\", 69000),\n",
        "\n",
        "]\n",
        "columns = ['name', 'department', 'salary']\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "# Defin window function and use LAG to compare the salary\n",
        "windowSpace = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "df_with_lag = df.withColumn(\"previous_salary\", lag(\"salary\").over(windowSpace))\n",
        "df_with_lag.show()\n",
        "\n",
        "#Apply Classification Logic\n",
        "final_df = df_with_lag.withColumn(\n",
        "    \"growth_status\",\n",
        "    when(col(\"previous_salary\").isNull(), \"No Growth\")\\\n",
        "    .when(col(\"salary\")>col(\"previous_salary\"), \"High Growth\")\\\n",
        "    .when(col(\"salary\")<col(\"previous_salary\"), \"Low Growth\")\\\n",
        "    .otherwise(\"No Growth\")\n",
        ")\n",
        "\n",
        "#Display the result\n",
        "final_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NOCxkBOM-AlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305c7a10-0ac1-4f10-beb9-4424fe591fe5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+\n",
            "|   name|department|salary|\n",
            "+-------+----------+------+\n",
            "|  Alice|        HR| 50000|\n",
            "|    Bob|        IT| 60000|\n",
            "|Charlie|        HR| 55000|\n",
            "|  David|        IT| 70000|\n",
            "|    Eve|        HR| 52000|\n",
            "|  Frank|        IT| 65000|\n",
            "|  Grace|        HR| 51000|\n",
            "|   Hank|        IT| 68000|\n",
            "|    Ivy|        HR| 53000|\n",
            "|   Jack|        IT| 72000|\n",
            "|  Kelly|        HR| 54000|\n",
            "|   Luke|        IT| 69000|\n",
            "+-------+----------+------+\n",
            "\n",
            "+-------+----------+------+---------------+\n",
            "|   name|department|salary|previous_salary|\n",
            "+-------+----------+------+---------------+\n",
            "|  Alice|        HR| 50000|           NULL|\n",
            "|  Grace|        HR| 51000|          50000|\n",
            "|    Eve|        HR| 52000|          51000|\n",
            "|    Ivy|        HR| 53000|          52000|\n",
            "|  Kelly|        HR| 54000|          53000|\n",
            "|Charlie|        HR| 55000|          54000|\n",
            "|    Bob|        IT| 60000|           NULL|\n",
            "|  Frank|        IT| 65000|          60000|\n",
            "|   Hank|        IT| 68000|          65000|\n",
            "|   Luke|        IT| 69000|          68000|\n",
            "|  David|        IT| 70000|          69000|\n",
            "|   Jack|        IT| 72000|          70000|\n",
            "+-------+----------+------+---------------+\n",
            "\n",
            "+-------+----------+------+---------------+-------------+\n",
            "|   name|department|salary|previous_salary|growth_status|\n",
            "+-------+----------+------+---------------+-------------+\n",
            "|  Alice|        HR| 50000|           NULL|    No Growth|\n",
            "|  Grace|        HR| 51000|          50000|  High Growth|\n",
            "|    Eve|        HR| 52000|          51000|  High Growth|\n",
            "|    Ivy|        HR| 53000|          52000|  High Growth|\n",
            "|  Kelly|        HR| 54000|          53000|  High Growth|\n",
            "|Charlie|        HR| 55000|          54000|  High Growth|\n",
            "|    Bob|        IT| 60000|           NULL|    No Growth|\n",
            "|  Frank|        IT| 65000|          60000|  High Growth|\n",
            "|   Hank|        IT| 68000|          65000|  High Growth|\n",
            "|   Luke|        IT| 69000|          68000|  High Growth|\n",
            "|  David|        IT| 70000|          69000|  High Growth|\n",
            "|   Jack|        IT| 72000|          70000|  High Growth|\n",
            "+-------+----------+------+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places. If a product does not have any sold units, its average selling price is assumed to be 0.\n"
      ],
      "metadata": {
        "id": "WbwcFFFqieVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "#Sample Data\n",
        "prices_data = [\n",
        "    (1, 100.0, \"2023-01-01\", \"2023-01-31\"),\n",
        "    (2, 200.0, \"2023-01-10\", \"2023-02-10\"),\n",
        "    (3, 150.0, \"2023-01-15\", \"2023-01-25\"),\n",
        "]\n",
        "\n",
        "units_sold_data =[\n",
        "    (1,\"2023-01-05\",10),\n",
        "    (2,\"2023-01-15\",5),\n",
        "    (2,\"2023-02-05\",5),\n",
        "    (4,\"2023-01-25\",5)\n",
        "]\n",
        "\n",
        "# Define Schema\n",
        "prices_column = ['product_id', 'price', 'start_date', 'end_date']\n",
        "units_sold_column = ['product_id', 'purchase_date', 'units']\n",
        "\n",
        "prices_df = spark.createDataFrame(prices_data, prices_column)\n",
        "units_sold_df = spark.createDataFrame(units_sold_data, units_sold_column)\n",
        "\n",
        "\n",
        "prices_df = prices_df.withColumn(\"start_data\", to_date(\"start_date\")).withColumn(\"end_date\", to_date(\"end_date\"))\n",
        "prices_df.show()\n",
        "\n",
        "units_sold_df = units_sold_df.withColumn(\"purchase_date\", to_date(\"purchase_date\"))\n",
        "units_sold_df.show()\n",
        "\n",
        "#Join wih date  filtering condition\n",
        "join_df = prices_df.join(units_sold_df, (prices_df.product_id == units_sold_df.product_id)&(units_sold_df.purchase_date.between(prices_df.start_date, prices_df.end_date)), \"left\")\n",
        "join_df.show()\n",
        "\n",
        "#Calculate total revenue and unit sold per product\n",
        "total_revenue_df = join_df.groupBy(prices_df.product_id)\\\n",
        "  .agg(\n",
        "      sum(col(\"price\") * col(\"units\")).alias(\"total_revenue\"),\n",
        "      sum(\"units\").alias(\"total_units\")\n",
        "  )\\\n",
        "  .withColumn(\"average_price\",\n",
        "      when(col(\"total_units\").isNull(), 0.0)\n",
        "      .otherwise(round(col(\"total_revenue\") / col(\"total_units\"), 2))\n",
        "  )\\\n",
        "  .select(\"product_id\", \"average_price\")\n",
        "\n",
        "total_revenue_df.show()"
      ],
      "metadata": {
        "id": "uAimnEH1zIG4",
        "outputId": "f9612809-77c0-4e89-9955-0a94e3e53f9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----------+----------+----------+\n",
            "|product_id|price|start_date|  end_date|start_data|\n",
            "+----------+-----+----------+----------+----------+\n",
            "|         1|100.0|2023-01-01|2023-01-31|2023-01-01|\n",
            "|         2|200.0|2023-01-10|2023-02-10|2023-01-10|\n",
            "|         3|150.0|2023-01-15|2023-01-25|2023-01-15|\n",
            "+----------+-----+----------+----------+----------+\n",
            "\n",
            "+----------+-------------+-----+\n",
            "|product_id|purchase_date|units|\n",
            "+----------+-------------+-----+\n",
            "|         1|   2023-01-05|   10|\n",
            "|         2|   2023-01-15|    5|\n",
            "|         2|   2023-02-05|    5|\n",
            "|         4|   2023-01-25|    5|\n",
            "+----------+-------------+-----+\n",
            "\n",
            "+----------+-----+----------+----------+----------+----------+-------------+-----+\n",
            "|product_id|price|start_date|  end_date|start_data|product_id|purchase_date|units|\n",
            "+----------+-----+----------+----------+----------+----------+-------------+-----+\n",
            "|         1|100.0|2023-01-01|2023-01-31|2023-01-01|         1|   2023-01-05|   10|\n",
            "|         3|150.0|2023-01-15|2023-01-25|2023-01-15|      NULL|         NULL| NULL|\n",
            "|         2|200.0|2023-01-10|2023-02-10|2023-01-10|         2|   2023-02-05|    5|\n",
            "|         2|200.0|2023-01-10|2023-02-10|2023-01-10|         2|   2023-01-15|    5|\n",
            "+----------+-----+----------+----------+----------+----------+-------------+-----+\n",
            "\n",
            "+----------+-------------+\n",
            "|product_id|average_price|\n",
            "+----------+-------------+\n",
            "|         1|        100.0|\n",
            "|         3|          0.0|\n",
            "|         2|        200.0|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hv7YbCG8pnvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}